{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook complete the last step of the data preprocessing before feeding the data to a model. These preprocessing includes:\n",
    "- Select the label to train\n",
    "- Train/Test Set Split\n",
    "- Tokenization\n",
    "- Token to Sequence\n",
    "- Pad the Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from importlib import reload\n",
    "from helpers import constants; reload(constants)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from helpers.helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some constants\n",
    "select_label = constants.SELECT_LABEL\n",
    "sample_rate = constants.SAMPLE_RATE\n",
    "\n",
    "raw_data_path = constants.RAW_TEXT_DIR\n",
    "base_data_path = constants.BASE_DATA_DIR\n",
    "intermediate_path = constants.ITM_DATA_DIR\n",
    "model_data_path = constants.PRCD_DATA_DIR\n",
    "tokenizer_path = constants.TOKEN_DIR\n",
    "sample_data_path = constants.SAMPLE_DATA_DIR\n",
    "\n",
    "max_len = constants.MAX_SEQUENCE_LENGTH # max number of words in a post to use\n",
    "max_word_no = constants.MAX_NUM_WORDS # how many unique words to use (i.e num rows in embedding vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "post_path = intermediate_path +'/post_df_short.pickle'\n",
    "df_raw = pd.read_pickle(path=post_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, ratio=0.8):\n",
    "    m = np.random.rand(len(df)) < ratio\n",
    "    train, test = df[m].copy(deep = True), df[~m].copy(deep = True)\n",
    "    x_train=train.post\n",
    "    y_train=train.target\n",
    "    x_test=test.post\n",
    "    y_test=test.target\n",
    "    return (x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, label, ratio=0.1):\n",
    "    num_full = data.shape[0]\n",
    "    if num_full != label.shape[0]:\n",
    "        return None\n",
    "    else:\n",
    "        num_sample = int(num_full*ratio)\n",
    "        m = (np.random.rand(num_sample)*num_full).astype(int)\n",
    "        sample_data = data[m]\n",
    "        sample_label = label[m]\n",
    "        return (sample_data, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_dataset(df_raw,'post', select_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello to anyone who's reading this. I've be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gosh, Jer. This is a blog. Not a chatroom. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey! people please come here!  I'm dying......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is that \"mandough\" the mando I know?  Or so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Omigodessess!!!! What the fork!  Hey, doofu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  target\n",
       "0     Hello to anyone who's reading this. I've be...       1\n",
       "1     Gosh, Jer. This is a blog. Not a chatroom. ...       1\n",
       "2     Hey! people please come here!  I'm dying......       1\n",
       "3     Is that \"mandough\" the mando I know?  Or so...       1\n",
       "4     Omigodessess!!!! What the fork!  Hey, doofu...       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = train_test_split(df, ratio=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to re-run this cell again if the fitted tokenizer object is pickled\n",
    "tokenizer = Tokenizer(num_words=max_word_no)\n",
    "tokenizer.fit_on_texts(x_train.tolist())\n",
    "\n",
    "# save fitted tokenizer so I don't have to train it every time\n",
    "with open(tokenizer_path, 'wb') as tokenizer_file:\n",
    "    pickle.dump(tokenizer, tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the fitted tokenizer from pickle\n",
    "with open(tokenizer_path, 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_train = tokenizer.texts_to_sequences(x_train.tolist()) # turn words into indices\n",
    "seq_test = tokenizer.texts_to_sequences(x_test.tolist()) # turn words into indices\n",
    "#seq_train and seq_test are two level embeded list[[...],[...],...[...]], each item is one text converted to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 520667 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index # a dictionary in the form of {'word': word_index,...}\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (449782, 500)\n",
      "Shape of training label: (449782, 2)\n"
     ]
    }
   ],
   "source": [
    "data_train = pad_sequences(seq_train, maxlen = max_len)\n",
    "data_test = pad_sequences(seq_test, maxlen = max_len)\n",
    "labels_train = to_categorical(np.asarray(y_train))\n",
    "labels_test = to_categorical(np.asarray(y_test))\n",
    "print('Shape of training data:', data_train.shape)\n",
    "print('Shape of training label:', labels_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sample for model test\n",
    "(data_train_sample, labels_train_sample), (data_test_sample, labels_test_sample) = sample_data(data_train, labels_train, ratio=sample_rate), sample_data(data_test, labels_test, ratio=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((data_train, labels_train, data_test, labels_test), open(model_data_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((data_train_sample, labels_train_sample, data_test_sample, labels_test_sample),open(sample_data_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x19b86e97978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
